{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use CIFAR 10 to train CNNs with Pytorch-lightning!\n",
    "\n",
    "let's develop 16FP and multi-GPU training.. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.models.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from models import *\n",
    "# from utils import progress_bar\n",
    "\n",
    "from test_tube import HyperOptArgumentParser, Experiment\n",
    "\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for Lightning Modules\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from test_tube import HyperOptArgumentParser\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.root_module.root_module import LightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Modelを定義する。\n",
    "おまじない的な側面も強い。Tutorialを読むのを推奨する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lightning model\n",
    "class LightningModel(LightningModule):\n",
    "    \"\"\"\n",
    "    Sample model to show how to define a template\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"\n",
    "        Pass in parsed HyperOptArgumentParser to the model\n",
    "        :param hparams:\n",
    "        \"\"\"\n",
    "        # init superclass\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.batch_size = hparams.batch_size\n",
    "\n",
    "        # if you specify an example input, the summary will show input/output for each layer\n",
    "        self.example_input_array = torch.rand(5, 3, 32, 32)\n",
    "\n",
    "        # build model\n",
    "        self.__build_model()\n",
    "\n",
    "    # ---------------------\n",
    "    # MODEL SETUP\n",
    "    # ---------------------\n",
    "    def __build_model(self):\n",
    "        \"\"\"\n",
    "        Layout model\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # inlayer\n",
    "        self.c1 = nn.Conv2d(3, 32, 3, padding=(1,1))\n",
    "        self.c2 = nn.Conv2d(32, 64, 3, padding=(1,1))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) # 16x16\n",
    "        self.c3 = nn.Conv2d(64, 64, 3, padding=(1,1))\n",
    "        self.c4 = nn.Conv2d(64, 64, 3, padding=(1,1))\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) # 8x8\n",
    "               \n",
    "        self.c_d1 = nn.Linear(in_features=8*8*64,\n",
    "                              out_features=self.hparams.hidden_dim)\n",
    "        self.c_d1_bn = nn.BatchNorm1d(self.hparams.hidden_dim)\n",
    "        self.c_d1_drop = nn.Dropout(self.hparams.drop_prob)\n",
    "        \n",
    "        self.c_d2 = nn.Linear(in_features=self.hparams.hidden_dim,\n",
    "                              out_features=self.hparams.out_features)\n",
    "\n",
    "    # ---------------------\n",
    "    # TRAINING\n",
    "    # ---------------------\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        No special modification required for lightning, define as you normally would\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = F.relu(self.c1(x))\n",
    "        x = F.relu(self.c2(x))\n",
    "        x = self.pool1(x) # 16\n",
    "        \n",
    "        x = F.relu(self.c3(x))\n",
    "        x = F.relu(self.c4(x))\n",
    "        x = self.pool2(x) # 8\n",
    "    \n",
    "        batch_size = x.size(0)     \n",
    "        x = F.relu(self.c_d1(x.view(batch_size, -1)))\n",
    "        #x = self.c_d1_bn(x)\n",
    "        #x = self.c_d1_drop(x)\n",
    "        \n",
    "        x = self.c_d2(x)\n",
    "        logits = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    # criterion的にロスを定義する？\n",
    "    def loss(self, labels, logits):\n",
    "        nll = F.nll_loss(logits, labels)\n",
    "        return nll\n",
    "    \n",
    "    # 学習のstepで何をやるか定義\n",
    "    def training_step(self, data_batch, batch_i):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        :param data_batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        x, y = data_batch\n",
    "        #x = x.view(x.size(0), -1) #全結合のためflatten\n",
    "\n",
    "        y_hat = self.forward(x) # forward\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.loss(y, y_hat)\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            'loss': loss_val\n",
    "        })\n",
    "\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "\n",
    "    # valで何をやるか定義する\n",
    "    def validation_step(self, data_batch, batch_i):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the validation loop\n",
    "        :param data_batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x, y = data_batch\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss_val = self.loss(y, y_hat)\n",
    "\n",
    "        # acc\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\n",
    "        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "\n",
    "        if self.on_gpu:\n",
    "            val_acc = val_acc.cuda(loss_val.device.index)\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "            val_acc = val_acc.unsqueeze(0)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            'val_loss': loss_val,\n",
    "            'val_acc': val_acc,\n",
    "        })\n",
    "\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Called at the end of validation to aggregate outputs\n",
    "        :param outputs: list of individual outputs of each validation step\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # if returned a scalar from validation_step, outputs is a list of tensor scalars\n",
    "        # we return just the average in this case (if we want)\n",
    "        # return torch.stack(outputs).mean()\n",
    "\n",
    "        val_loss_mean = 0\n",
    "        val_acc_mean = 0\n",
    "        for output in outputs:\n",
    "            val_loss = output['val_loss']\n",
    "\n",
    "            # reduce manually when using dp\n",
    "            if self.trainer.use_dp:\n",
    "                val_loss = torch.mean(val_loss)\n",
    "            val_loss_mean += val_loss\n",
    "\n",
    "            # reduce manually when using dp\n",
    "            val_acc = output['val_acc']\n",
    "            if self.trainer.use_dp:\n",
    "                val_acc = torch.mean(val_acc)\n",
    "\n",
    "            val_acc_mean += val_acc\n",
    "\n",
    "        val_loss_mean /= len(outputs)\n",
    "        val_acc_mean /= len(outputs)\n",
    "        tqdm_dic = {'val_loss': val_loss_mean, 'val_acc': val_acc_mean}\n",
    "        return tqdm_dic\n",
    "\n",
    "    # ---------------------\n",
    "    # TRAINING SETUP\n",
    "    # ---------------------\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        return whatever optimizers we want here\n",
    "        :return: list of optimizers\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    # ここでデータローダーを定義する！\n",
    "    def __dataloader(self, train):\n",
    "        # init data generators\n",
    "        print('==> Preparing data..')\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        \n",
    "        if train:\n",
    "            dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "        else:\n",
    "            dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "        # when using multi-node (ddp) we need to add the datasampler\n",
    "        train_sampler = None\n",
    "        batch_size = self.hparams.batch_size\n",
    "\n",
    "        if self.trainer.use_ddp:\n",
    "            train_sampler = DistributedSampler(dataset, rank=self.trainer.proc_rank)\n",
    "            batch_size = batch_size // self.trainer.world_size  # scale batch size\n",
    "\n",
    "        should_shuffle = train_sampler is None\n",
    "        loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=should_shuffle,\n",
    "            sampler=train_sampler\n",
    "        )\n",
    "\n",
    "        return loader\n",
    "\n",
    "    @pl.data_loader\n",
    "    def tng_dataloader(self):\n",
    "        print('tng data loader called')\n",
    "        return self.__dataloader(train=True)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        print('val data loader called')\n",
    "        return self.__dataloader(train=False)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        print('test data loader called')\n",
    "        return self.__dataloader(train=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser, root_dir):  # pragma: no cover\n",
    "        \"\"\"\n",
    "        Parameters you define here will be available to your model through self.hparams\n",
    "        :param parent_parser:\n",
    "        :param root_dir:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        parser = HyperOptArgumentParser(strategy=parent_parser.strategy, parents=[parent_parser])\n",
    "\n",
    "        # param overwrites\n",
    "        # parser.set_defaults(gradient_clip=5.0)\n",
    "\n",
    "        # network params\n",
    "        parser.add_argument('--in_features', default=3 * 32 * 32, type=int)\n",
    "        parser.add_argument('--out_features', default=10, type=int)\n",
    "        # use 500 for CPU, 50000 for GPU to see speed difference\n",
    "        parser.add_argument('--hidden_dim', default=5000, type=int)\n",
    "        parser.opt_list('--drop_prob', default=0.2, options=[0.2, 0.5], type=float, tunable=False)\n",
    "\n",
    "        # data\n",
    "        parser.add_argument('--data_root', default=os.path.join(root_dir, 'mnist'), type=str)\n",
    "\n",
    "        # training params (opt)\n",
    "        parser.opt_list('--learning_rate', default=0.001 * 8, type=float,\n",
    "                        options=[0.0001, 0.0005, 0.001, 0.005],\n",
    "                        tunable=False)\n",
    "        parser.opt_list('--optimizer_name', default='adam', type=str,\n",
    "                        options=['adam'], tunable=False)\n",
    "\n",
    "        # if using 2 nodes with 4 gpus each the batch size here\n",
    "        #  (256) will be 256 / (2*8) = 16 per gpu\n",
    "        parser.opt_list('--batch_size', default=256, type=int,\n",
    "                        options=[32, 64, 128, 256], tunable=False,\n",
    "                        help='batch size will be divided over all gpus being used across all nodes')\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# argparseをLightningModelに渡す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# although we user hyperOptParser, we are using it only as argparse right now\n",
    "parent_parser = HyperOptArgumentParser(strategy='grid_search', add_help=False)\n",
    "\n",
    "# dirs\n",
    "root_dir = os.path.dirname(os.path.realpath(\".\"))\n",
    "demo_log_dir = os.path.join(root_dir, 'pt_lightning_demo_logs')\n",
    "checkpoint_dir = os.path.join(demo_log_dir, 'model_weights')\n",
    "test_tube_dir = os.path.join(demo_log_dir, 'test_tube_data')\n",
    "\n",
    "# gpu args\n",
    "parent_parser.add_argument('--gpus', type=str, default='-1',\n",
    "                               help='how many gpus to use in the node.'\n",
    "                                    'value -1 uses all the gpus on the node')\n",
    "parent_parser.add_argument('--test_tube_save_path', type=str,\n",
    "                           default=test_tube_dir, help='where to save logs')\n",
    "parent_parser.add_argument('--model_save_path', type=str,\n",
    "                           default=checkpoint_dir, help='where to save model')\n",
    "parent_parser.add_argument('--experiment_name', type=str,\n",
    "                           default='pt_lightning_exp_a', help='test tube exp name')\n",
    "\n",
    "# allow model to overwrite or extend args\n",
    "parser = LightningModel.add_model_specific_args(parent_parser, root_dir)\n",
    "hyperparams = parser.parse_args(args=[]) # for jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルを初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n",
      "build model\n"
     ]
    }
   ],
   "source": [
    "# init lightning model\n",
    "print(\"load model\")\n",
    "model = LightningModel(hyperparams)\n",
    "print(\"build model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentを初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init experiment\n",
    "exp = Experiment(\n",
    "        name=hyperparams.experiment_name,\n",
    "        save_dir=hyperparams.test_tube_save_path,\n",
    "        autosave=False,\n",
    "        description='test demo'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Callbacks\n",
    "Kerasのように定義できる！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '{}/{}/{}'.format(hyperparams.model_save_path, exp.name, exp.version)\n",
    "early_stop = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=model_save_path,\n",
    "        save_best_only=True,\n",
    "        verbose=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainerを初期化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISIBLE GPUS: '0'\n",
      "gpu available: True, used: True\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        experiment=exp,\n",
    "        checkpoint_callback=checkpoint,\n",
    "        early_stop_callback=early_stop,\n",
    "        gpus=hyperparams.gpus\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習を開始！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tng data loader called\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "test data loader called\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "val data loader called\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [2560 x 8], m2: [4096 x 5000] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-45d4afebefac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__single_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# ON CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__single_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__run_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# print model summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_weights_summary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0mref_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;31m# give model convenience properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/root_module/root_module.py\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mmodel_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/root_module/memory.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/root_module/memory.py\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_input_array\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/root_module/memory.py\u001b[0m in \u001b[0;36mget_variable_sizes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [2560 x 8], m2: [4096 x 5000] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CNN(nn.Module): \n",
    "    def __init__(self, bs):\n",
    "        \"\"\"\n",
    "        Pass in parsed HyperOptArgumentParser to the model\n",
    "        :param hparams:\n",
    "        \"\"\"\n",
    "        # init superclass\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "\n",
    "        # if you specify an example input, the summary will show input/output for each layer\n",
    "        self.example_input_array = torch.rand(5, 3, 32, 32)\n",
    "\n",
    "        # build model\n",
    "        self.__build_model()\n",
    "\n",
    "    # ---------------------\n",
    "    # MODEL SETUP\n",
    "    # ---------------------\n",
    "    def __build_model(self):\n",
    "        \"\"\"\n",
    "        Layout model\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # inlayer\n",
    "        self.c1 = nn.Conv2d(3, 32, 3, padding=(1,1))\n",
    "        self.c2 = nn.Conv2d(32, 64, 3, padding=(1,1))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) # 16x16\n",
    "        self.c3 = nn.Conv2d(64, 64, 3, padding=(1,1))\n",
    "        self.c4 = nn.Conv2d(64, 64, 3, padding=(1,1))\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) # 8x8\n",
    "               \n",
    "        self.c_d1 = nn.Linear(in_features=8*8*64,\n",
    "                              out_features=128)\n",
    "        self.c_d1_bn = nn.BatchNorm1d(128)\n",
    "        self.c_d1_drop = nn.Dropout(0.5)\n",
    "        \n",
    "        self.c_d2 = nn.Linear(in_features=128,\n",
    "                              out_features=10)\n",
    "\n",
    "    # ---------------------\n",
    "    # TRAINING\n",
    "    # ---------------------\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        No special modification required for lightning, define as you normally would\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = F.relu(self.c1(x))\n",
    "        x = F.relu(self.c2(x))\n",
    "        x = self.pool1(x) # 16\n",
    "        \n",
    "        x = F.relu(self.c3(x))\n",
    "        x = F.relu(self.c4(x))\n",
    "        x = self.pool2(x) # 8\n",
    "    \n",
    "        batch_size = x.size(0)     \n",
    "        x = F.relu(self.c_d1(x.view(batch_size, -1)))\n",
    "        x = self.c_d1_bn(x)\n",
    "        x = self.c_d1_drop(x)\n",
    "        \n",
    "        x = self.c_d2(x)\n",
    "        logits = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return logits\n",
    "nn = CNN(8)\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
