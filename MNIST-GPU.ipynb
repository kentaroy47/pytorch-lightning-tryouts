{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use CIFAR 10 to train CNNs with Pytorch-lightning!\n",
    "\n",
    "let's develop 16FP and multi-GPU training.. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.models.trainer import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from models import *\n",
    "# from utils import progress_bar\n",
    "\n",
    "from test_tube import HyperOptArgumentParser, Experiment\n",
    "\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for Lightning Modules\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from test_tube import HyperOptArgumentParser\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.root_module.root_module import LightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Modelを定義する。\n",
    "おまじない的な側面も強い。Tutorialを読むのを推奨する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lightning model\n",
    "class LightningModel(LightningModule):\n",
    "    \"\"\"\n",
    "    Sample model to show how to define a template\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"\n",
    "        Pass in parsed HyperOptArgumentParser to the model\n",
    "        :param hparams:\n",
    "        \"\"\"\n",
    "        # init superclass\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.batch_size = hparams.batch_size\n",
    "\n",
    "        # if you specify an example input, the summary will show input/output for each layer\n",
    "        self.example_input_array = torch.rand(5, 28 * 28)\n",
    "\n",
    "        # build model\n",
    "        self.__build_model()\n",
    "\n",
    "    # ---------------------\n",
    "    # MODEL SETUP\n",
    "    # ---------------------\n",
    "    def __build_model(self):\n",
    "        \"\"\"\n",
    "        Layout model\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # inlayer\n",
    "        self.c_d1 = nn.Linear(in_features=self.hparams.in_features,\n",
    "                              out_features=self.hparams.hidden_dim)\n",
    "        self.c_d1_bn = nn.BatchNorm1d(self.hparams.hidden_dim)\n",
    "        self.c_d1_drop = nn.Dropout(self.hparams.drop_prob)\n",
    "        # midlayer\n",
    "        #self.c_dm = nn.Linear(in_features=self.hparams.hidden_dim,\n",
    "        #                      out_features=self.hparams.hidden_dim)\n",
    "        #self.c_dm_bn = nn.BatchNorm1d(self.hparams.hidden_dim)\n",
    "        #self.c_dm_drop = nn.Dropout(self.hparams.drop_prob)\n",
    "        # outlayer\n",
    "        self.c_d2 = nn.Linear(in_features=self.hparams.hidden_dim,\n",
    "                              out_features=self.hparams.out_features)\n",
    "\n",
    "    # ---------------------\n",
    "    # TRAINING\n",
    "    # ---------------------\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        No special modification required for lightning, define as you normally would\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.c_d1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.c_d1_bn(x)\n",
    "        x = self.c_d1_drop(x)\n",
    "        \n",
    "#        x = self.c_dm(x)\n",
    "#        x = torch.relu(x)\n",
    "#        x = self.c_dm_bn(x)\n",
    "#        x = self.c_dm_drop(x)\n",
    "        \n",
    "        x = self.c_d2(x)\n",
    "        logits = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    # criterion的にロスを定義する？\n",
    "    def loss(self, labels, logits):\n",
    "        nll = F.nll_loss(logits, labels)\n",
    "        return nll\n",
    "    \n",
    "    # 学習のstepで何をやるか定義\n",
    "    def training_step(self, data_batch, batch_i):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        :param data_batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        x, y = data_batch\n",
    "        x = x.view(x.size(0), -1) #全結合のためflatten\n",
    "\n",
    "        y_hat = self.forward(x) # forward\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.loss(y, y_hat)\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            'loss': loss_val\n",
    "        })\n",
    "\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "\n",
    "    # valで何をやるか定義する\n",
    "    def validation_step(self, data_batch, batch_i):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the validation loop\n",
    "        :param data_batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x, y = data_batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        loss_val = self.loss(y, y_hat)\n",
    "\n",
    "        # acc\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\n",
    "        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "\n",
    "        if self.on_gpu:\n",
    "            val_acc = val_acc.cuda(loss_val.device.index)\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "            val_acc = val_acc.unsqueeze(0)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            'val_loss': loss_val,\n",
    "            'val_acc': val_acc,\n",
    "        })\n",
    "\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Called at the end of validation to aggregate outputs\n",
    "        :param outputs: list of individual outputs of each validation step\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # if returned a scalar from validation_step, outputs is a list of tensor scalars\n",
    "        # we return just the average in this case (if we want)\n",
    "        # return torch.stack(outputs).mean()\n",
    "\n",
    "        val_loss_mean = 0\n",
    "        val_acc_mean = 0\n",
    "        for output in outputs:\n",
    "            val_loss = output['val_loss']\n",
    "\n",
    "            # reduce manually when using dp\n",
    "            if self.trainer.use_dp:\n",
    "                val_loss = torch.mean(val_loss)\n",
    "            val_loss_mean += val_loss\n",
    "\n",
    "            # reduce manually when using dp\n",
    "            val_acc = output['val_acc']\n",
    "            if self.trainer.use_dp:\n",
    "                val_acc = torch.mean(val_acc)\n",
    "\n",
    "            val_acc_mean += val_acc\n",
    "\n",
    "        val_loss_mean /= len(outputs)\n",
    "        val_acc_mean /= len(outputs)\n",
    "        tqdm_dic = {'val_loss': val_loss_mean, 'val_acc': val_acc_mean}\n",
    "        return tqdm_dic\n",
    "\n",
    "    # ---------------------\n",
    "    # TRAINING SETUP\n",
    "    # ---------------------\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        return whatever optimizers we want here\n",
    "        :return: list of optimizers\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    # ここでデータローダーを定義する！\n",
    "    def __dataloader(self, train):\n",
    "        # init data generators\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5,), (1.0,))])\n",
    "        dataset = MNIST(root=self.hparams.data_root, train=train,\n",
    "                        transform=transform, download=True)\n",
    "\n",
    "        # when using multi-node (ddp) we need to add the datasampler\n",
    "        train_sampler = None\n",
    "        batch_size = self.hparams.batch_size\n",
    "\n",
    "        if self.trainer.use_ddp:\n",
    "            train_sampler = DistributedSampler(dataset, rank=self.trainer.proc_rank)\n",
    "            batch_size = batch_size // self.trainer.world_size  # scale batch size\n",
    "\n",
    "        should_shuffle = train_sampler is None\n",
    "        loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=should_shuffle,\n",
    "            sampler=train_sampler\n",
    "        )\n",
    "\n",
    "        return loader\n",
    "\n",
    "    @pl.data_loader\n",
    "    def tng_dataloader(self):\n",
    "        print('tng data loader called')\n",
    "        return self.__dataloader(train=True)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        print('val data loader called')\n",
    "        return self.__dataloader(train=False)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        print('test data loader called')\n",
    "        return self.__dataloader(train=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser, root_dir):  # pragma: no cover\n",
    "        \"\"\"\n",
    "        Parameters you define here will be available to your model through self.hparams\n",
    "        :param parent_parser:\n",
    "        :param root_dir:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        parser = HyperOptArgumentParser(strategy=parent_parser.strategy, parents=[parent_parser])\n",
    "\n",
    "        # param overwrites\n",
    "        # parser.set_defaults(gradient_clip=5.0)\n",
    "\n",
    "        # network params\n",
    "        parser.add_argument('--in_features', default=28 * 28, type=int)\n",
    "        parser.add_argument('--out_features', default=10, type=int)\n",
    "        # use 500 for CPU, 50000 for GPU to see speed difference\n",
    "        parser.add_argument('--hidden_dim', default=50000, type=int)\n",
    "        parser.opt_list('--drop_prob', default=0.2, options=[0.2, 0.5], type=float, tunable=False)\n",
    "\n",
    "        # data\n",
    "        parser.add_argument('--data_root', default=os.path.join(root_dir, 'mnist'), type=str)\n",
    "\n",
    "        # training params (opt)\n",
    "        parser.opt_list('--learning_rate', default=0.001 * 8, type=float,\n",
    "                        options=[0.0001, 0.0005, 0.001, 0.005],\n",
    "                        tunable=False)\n",
    "        parser.opt_list('--optimizer_name', default='adam', type=str,\n",
    "                        options=['adam'], tunable=False)\n",
    "\n",
    "        # if using 2 nodes with 4 gpus each the batch size here\n",
    "        #  (256) will be 256 / (2*8) = 16 per gpu\n",
    "        parser.opt_list('--batch_size', default=256 * 8, type=int,\n",
    "                        options=[32, 64, 128, 256], tunable=False,\n",
    "                        help='batch size will be divided over all gpus being used across all nodes')\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# argparseをLightningModelに渡す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# although we user hyperOptParser, we are using it only as argparse right now\n",
    "parent_parser = HyperOptArgumentParser(strategy='grid_search', add_help=False)\n",
    "\n",
    "# dirs\n",
    "root_dir = os.path.dirname(os.path.realpath(\".\"))\n",
    "demo_log_dir = os.path.join(root_dir, 'pt_lightning_demo_logs')\n",
    "checkpoint_dir = os.path.join(demo_log_dir, 'model_weights')\n",
    "test_tube_dir = os.path.join(demo_log_dir, 'test_tube_data')\n",
    "\n",
    "# gpu args\n",
    "parent_parser.add_argument('--gpus', type=str, default='-1',\n",
    "                               help='how many gpus to use in the node.'\n",
    "                                    'value -1 uses all the gpus on the node')\n",
    "parent_parser.add_argument('--test_tube_save_path', type=str,\n",
    "                           default=test_tube_dir, help='where to save logs')\n",
    "parent_parser.add_argument('--model_save_path', type=str,\n",
    "                           default=checkpoint_dir, help='where to save model')\n",
    "parent_parser.add_argument('--experiment_name', type=str,\n",
    "                           default='pt_lightning_exp_a', help='test tube exp name')\n",
    "\n",
    "# allow model to overwrite or extend args\n",
    "parser = LightningModel.add_model_specific_args(parent_parser, root_dir)\n",
    "hyperparams = parser.parse_args(args=[]) # for jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルを初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n",
      "build model\n"
     ]
    }
   ],
   "source": [
    "# init lightning model\n",
    "print(\"load model\")\n",
    "model = LightningModel(hyperparams)\n",
    "print(\"build model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentを初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init experiment\n",
    "exp = Experiment(\n",
    "        name=hyperparams.experiment_name,\n",
    "        save_dir=hyperparams.test_tube_save_path,\n",
    "        autosave=False,\n",
    "        description='test demo'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Callbacks\n",
    "Kerasのように定義できる！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '{}/{}/{}'.format(hyperparams.model_save_path, exp.name, exp.version)\n",
    "early_stop = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=3,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=model_save_path,\n",
    "        save_best_only=True,\n",
    "        verbose=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainerを初期化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISIBLE GPUS: '0'\n",
      "gpu available: True, used: True\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        experiment=exp,\n",
    "        checkpoint_callback=checkpoint,\n",
    "        early_stop_callback=early_stop,\n",
    "        gpus=hyperparams.gpus\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習を開始！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tng data loader called\n",
      "test data loader called\n",
      "val data loader called\n",
      "        Name         Type    Params    In_sizes   Out_sizes\n",
      "0       c_d1       Linear  39250000    [5, 784]  [5, 50000]\n",
      "1    c_d1_bn  BatchNorm1d    100000  [5, 50000]  [5, 50000]\n",
      "2  c_d1_drop      Dropout         0  [5, 50000]  [5, 50000]\n",
      "3       c_d2       Linear    500010  [5, 50000]     [5, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.05it/s, batch_nb=27, epoch=0, gpu=0, tng_loss=5.086, v_nb=12, val_acc=0.263, val_loss=33.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 33.91142, saving model to /home/ubuntu/pt_lightning_demo_logs/model_weights/pt_lightning_exp_a/12/_ckpt_epoch_1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.05it/s, batch_nb=27, epoch=1, gpu=0, tng_loss=2.534, v_nb=12, val_acc=0.711, val_loss=1.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00002: val_loss improved from 33.91142 to 1.63791, saving model to /home/ubuntu/pt_lightning_demo_logs/model_weights/pt_lightning_exp_a/12/_ckpt_epoch_2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.06it/s, batch_nb=27, epoch=2, gpu=0, tng_loss=1.698, v_nb=12, val_acc=0.959, val_loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.63791 to 0.15380, saving model to /home/ubuntu/pt_lightning_demo_logs/model_weights/pt_lightning_exp_a/12/_ckpt_epoch_3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.07it/s, batch_nb=27, epoch=3, gpu=0, tng_loss=0.126, v_nb=12, val_acc=0.963, val_loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15380 to 0.13320, saving model to /home/ubuntu/pt_lightning_demo_logs/model_weights/pt_lightning_exp_a/12/_ckpt_epoch_4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  2.98it/s, batch_nb=27, epoch=4, gpu=0, tng_loss=0.070, v_nb=12, val_acc=0.957, val_loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.06it/s, batch_nb=27, epoch=5, gpu=0, tng_loss=0.052, v_nb=12, val_acc=0.974, val_loss=0.0861]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13320 to 0.08612, saving model to /home/ubuntu/pt_lightning_demo_logs/model_weights/pt_lightning_exp_a/12/_ckpt_epoch_6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.06it/s, batch_nb=27, epoch=6, gpu=0, tng_loss=0.037, v_nb=12, val_acc=0.979, val_loss=0.0779]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08612 to 0.07790, saving model to /home/ubuntu/pt_lightning_demo_logs/model_weights/pt_lightning_exp_a/12/_ckpt_epoch_7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.08it/s, batch_nb=27, epoch=7, gpu=0, tng_loss=0.025, v_nb=12, val_acc=0.982, val_loss=0.063] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07790 to 0.06301, saving model to /home/ubuntu/pt_lightning_demo_logs/model_weights/pt_lightning_exp_a/12/_ckpt_epoch_8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.04it/s, batch_nb=27, epoch=8, gpu=0, tng_loss=0.017, v_nb=12, val_acc=0.982, val_loss=0.0626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06301 to 0.06264, saving model to /home/ubuntu/pt_lightning_demo_logs/model_weights/pt_lightning_exp_a/12/_ckpt_epoch_9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.07it/s, batch_nb=27, epoch=9, gpu=0, tng_loss=0.012, v_nb=12, val_acc=0.983, val_loss=0.0587]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06264 to 0.05869, saving model to /home/ubuntu/pt_lightning_demo_logs/model_weights/pt_lightning_exp_a/12/_ckpt_epoch_10.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.08it/s, batch_nb=27, epoch=10, gpu=0, tng_loss=0.010, v_nb=12, val_acc=0.983, val_loss=0.0591]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.03it/s, batch_nb=27, epoch=11, gpu=0, tng_loss=0.009, v_nb=12, val_acc=0.983, val_loss=0.0591]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:12<00:00,  3.02it/s, batch_nb=27, epoch=12, gpu=0, tng_loss=0.009, v_nb=12, val_acc=0.983, val_loss=0.0593]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.07it/s, batch_nb=27, epoch=13, gpu=0, tng_loss=0.009, v_nb=12, val_acc=0.982, val_loss=0.0642]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 33/35 [00:11<00:00,  3.01it/s, batch_nb=27, epoch=14, gpu=0, tng_loss=0.009, v_nb=12, val_acc=0.979, val_loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save callback...\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:12<00:00,  3.71it/s, batch_nb=29, epoch=14, gpu=0, tng_loss=0.009, v_nb=12, val_acc=0.979, val_loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
